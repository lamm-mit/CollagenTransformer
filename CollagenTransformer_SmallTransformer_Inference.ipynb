{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b582b69",
   "metadata": {},
   "source": [
    "# CollagenTransformer: End-to-end transformer model to predict thermal stability of collagen triple helices using an NLP approach \n",
    "\n",
    "#### Eesha Khare1,2, Constancio Gonzalez Obeso3, David L. Kaplan3 Markus J. Buehler1,4*\n",
    "\n",
    "1 Laboratory for Atomistic and Molecular Mechanics (LAMM), Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA\n",
    "2 Department of Materials Science and Engineering, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA\n",
    "3 Tufts University, Medford, MA, USA\n",
    "4 Center for Computational Science and Engineering, Schwarzman College of Computing, Massachusetts Institute of Technology, 77 Massachusetts Ave., Cambridge, MA 02139, USA\n",
    "\n",
    "*mbuehler@mit.edu "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f435b0",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "73ec2b9f",
    "outputId": "145a6fe6-60e8-42b0-c8a4-65ce18792689"
   },
   "source": [
    "## Collagen transformer model to predict Tm (small transformer model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06c33765",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7a93794f",
    "outputId": "84615f25-ee27-4f8b-cd1a-b602e9b9c3b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marku\\anaconda3\\envs\\Huggingface New\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch version: 1.11.0+cu113\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os,sys\n",
    "import math\n",
    "#os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import cv2\n",
    "import PIL \n",
    "from PIL import Image, ImageOps\n",
    "\n",
    "import pickle\n",
    "import torchvision.utils as vutils\n",
    "import torch\n",
    " \n",
    "import torchvision\n",
    " \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    " \n",
    "from torch import nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ExponentialLR, StepLR\n",
    "\n",
    "print(\"Torch version:\", torch.__version__) \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "\n",
    "import seaborn as sns\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d7a3037c",
   "metadata": {
    "id": "83162d32"
   },
   "outputs": [],
   "source": [
    "from math import pi, log\n",
    "import time\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "from sklearn.preprocessing import RobustScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c000977",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "!wget https://www.dropbox.com/s/8oiyjvgn3nofkkh/model_IO_REGRESS_1290.pth?dl=0 -O model_IO_REGRESS_1290.pth\n",
    "!wget https://www.dropbox.com/s/60ximulejl6fq4b/tokenizer_and_qt.pickle?dl=0 -O tokenizer_and_qt.pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08fbd8eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#source: https://github.com/lucidrains/x-transformers\n",
    "#!pip install x_transformers==0.28.0\n",
    "\n",
    "from x_transformers import XTransformer,ContinuousTransformerWrapper, Decoder, Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5fd5013",
   "metadata": {
    "id": "b63beb53"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() \n",
    "                                  else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd0fa4ac",
   "metadata": {
    "id": "67f1232b"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader,Dataset\n",
    "from torchvision.io import read_image\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "to_pil = transforms.ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b13b6544",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b77b96b9",
    "outputId": "cfe03e1b-e612-4531-980f-1df7ff6b1feb"
   },
   "outputs": [],
   "source": [
    "num_tokens_=25 \n",
    "max_length = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "30ffbd59",
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_dim_x = 8  # for x\n",
    "embed_dim = 8 #for position \n",
    "embed_dim_conv=8  #for conv\n",
    "kernel_siz = 3\n",
    "\n",
    "latent_dim_ =32\n",
    "\n",
    "nconv=2 #number of convs for input\n",
    "\n",
    "class MyTotal_pos(nn.Module):\n",
    "    def __init__(self ):\n",
    "        super(MyTotal_pos, self).__init__()\n",
    "        \n",
    "        self.mSig = nn.Sigmoid()\n",
    "        self.queries_dim_=embed_dim  + embed_dim_x #dim_\n",
    "         \n",
    "         \n",
    "        \n",
    "        self.emb_data = nn.Embedding(num_tokens_, embed_dim_x*1,  padding_idx=0)\n",
    "        self.pos_emb_x = nn.Embedding(max_length, embed_dim*1)\n",
    "        \n",
    "        self.fc_x1 = nn.Linear(   embed_dim_x, embed_dim_x )  # INPUT DIM (last), OUTPUT DIM, last\n",
    "        \n",
    "        self.fc1 = nn.Linear( max_length,  1)  # INPUT DIM (last), OUTPUT DIM, last\n",
    "        self.fc2 = nn.Linear( max_length,  1)  # INPUT DIM (last), OUTPUT DIM, last\n",
    "        self.fc3 = nn.Linear( max_length,  1)  # INPUT DIM (last), OUTPUT DIM, last\n",
    "        \n",
    "        self.BatchNorm_1 = nn.BatchNorm1d(max_length)\n",
    "        self.fc_last = nn.Linear( self.queries_dim_,  1)  \n",
    "        self.fc_last_2 = nn.Linear( max_length,  1)  \n",
    "        \n",
    "        self.convs=[]\n",
    "        self.skips_cons=[]\n",
    "        icgg=0\n",
    "        for _ in range(nconv):\n",
    "            if icgg==0: #first one opertes on 3 channels only\n",
    "                self.convs.append(nn.Conv1d(in_channels=embed_dim_x, out_channels=embed_dim_conv, kernel_size=3, stride=1, \\\n",
    "                                        padding='same'))#.cuda())\n",
    "                 \n",
    "            if icgg>0:\n",
    "                self.convs.append(nn.Conv1d(in_channels=embed_dim_x, out_channels=embed_dim_conv, kernel_size=3, stride=1, \\\n",
    "                                        padding='same'))#.cuda())\n",
    "            icgg=icgg+1\n",
    "             \n",
    "        self.convs = nn.ModuleList(self.convs)\n",
    "        \n",
    "        self.pos_matrix_i = torch.zeros (max_length, dtype=torch.long)\n",
    "        \n",
    "        for i in range (max_length):\n",
    "            \n",
    "                self.pos_matrix_i [i]=i\n",
    "            \n",
    " \n",
    "        self.modelB = ContinuousTransformerWrapper(\n",
    "            dim_in = self.queries_dim_,\n",
    "            dim_out = self.queries_dim_,\n",
    "            max_seq_len = max_length,\n",
    "            attn_layers = Encoder(\n",
    "                dim = self.queries_dim_,\n",
    "                depth = 3,\n",
    "               heads = 8\n",
    "               ))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        n=self.queries_dim_\n",
    "         \n",
    "        x = self.emb_data( x)\n",
    "       \n",
    "        x = torch.squeeze(x, 1)\n",
    "        \n",
    "        x= self.BatchNorm_1(x)\n",
    "        \n",
    "        pos_matrix_i_=self.pos_matrix_i.repeat(x.shape[0], 1, 1).to(device=device) \n",
    "        \n",
    "        \n",
    "        pos_emb_x = self.pos_emb_x( pos_matrix_i_)\n",
    "        pos_emb_x = torch.squeeze(pos_emb_x, 1)\n",
    "        \n",
    "        self.skips_cons=[]\n",
    "         \n",
    "        convcount=0\n",
    "        for  encoder in self.convs:\n",
    "            if convcount==0:\n",
    "                x_cc=torch.permute(x, (0,2,1)  )\n",
    "            else:\n",
    "                x_cc=torch.permute(x_cc, (0,2,1)  )\n",
    "            \n",
    "            x_cc =  encoder.to(device=device)(x_cc)\n",
    "            x_cc=torch.permute(x_cc, (0,2,1)  )\n",
    "           \n",
    "            self.skips_cons.append(x_cc)\n",
    "            convcount=  convcount+1\n",
    "            \n",
    "        \n",
    "        for skip in self.skips_cons[:-1]:\n",
    "            x_cc = torch.cat((x_cc, skip), axis=2)\n",
    "\n",
    "        \n",
    "        x= torch.cat( (x,    pos_emb_x), 2)\n",
    "             \n",
    "        x2 = self.modelB(x) #, queries = query1)\n",
    "        \n",
    "        x2=self.fc_last (x2)\n",
    "        x2=torch.permute(x2, (0,2,1)  )\n",
    "        x2=self.fc_last_2 (x2)\n",
    "        #x2=self.mSig(x2)\n",
    "        \n",
    "        return x2\n",
    " \n",
    "model = MyTotal_pos(  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d622ddf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:  108069  trainable parameters:  108069\n"
     ]
    }
   ],
   "source": [
    "pytorch_total_params = sum(p.numel() for p in model.parameters())\n",
    "pytorch_total_params_trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print (\"Total parameters: \", pytorch_total_params,\" trainable parameters: \", pytorch_total_params_trainable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8d024018",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading\n",
    "with open('tokenizer_and_qt.pickle', 'rb') as handle:\n",
    "    tokenizer = pickle.load(handle)\n",
    "    qt = pickle.load(handle)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0630f1a2",
   "metadata": {},
   "source": [
    "### General inference for a given sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "33ab0d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing import text, sequence\n",
    "def SS_predictor(model,tokenizer,seq,max_length, qt=None):\n",
    "    model.eval()\n",
    "    prot_1 = tokenizer.texts_to_sequences([seq[:]])\n",
    "    prot_1 = sequence.pad_sequences(prot_1, maxlen=max_length, padding='post' )\n",
    "    prot_1 = torch.from_numpy(prot_1).to(device)\n",
    "    \n",
    "    #print(prot_1.shape)\n",
    "    prot_tm=model(prot_1) \n",
    "    if qt : #inverse scale if PRESENT\n",
    "        temp_=prot_tm.cpu().detach().numpy() \n",
    "        temp_=np.squeeze(temp_, 2)\n",
    "        prot_tm=qt.inverse_transform(temp_)\n",
    "    \n",
    "    prot_tm=np.squeeze(prot_tm)\n",
    "    \n",
    "    return prot_tm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78e4d768",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load model if not loaded yet\n",
    "name='./model_IO_REGRESS_1290.pth'\n",
    "model = torch.load(name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82930ff2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.49929\n"
     ]
    }
   ],
   "source": [
    "Tm=SS_predictor (model, tokenizer, 'GPOGPOGPOGPOGPQGPGGPPGPOGPOGPOGPOGPO', max_length, qt)\n",
    "print (Tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "56b2f5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-6.225907\n"
     ]
    }
   ],
   "source": [
    "Tm=SS_predictor (model, tokenizer, 'GPOGPOGPODPOGPOGPOGPOGPO', max_length, qt)\n",
    "print (Tm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47452d2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "SecStructure_perceiver_17.ipynb",
   "provenance": []
  },
  "environment": {
   "kernel": "python3",
   "name": "pytorch-gpu.1-10.m89",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-10:m89"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
